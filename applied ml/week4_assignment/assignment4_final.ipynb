{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3198b16-cc92-43a4-a4df-74c1008744e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### MAKE SURE TO SWITCH DATA SET URLS BEFORE SUBMITTING \n",
    "\n",
    "def blight_model():\n",
    "    # load libraries & presets\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Your code here\n",
    "    import string\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    #load datasets\n",
    "    train_data = pd.read_csv(\"train.csv\", sep=\",\", encoding='cp1252')\n",
    "    test_data = pd.read_csv(\"test.csv\", sep=\",\", encoding='cp1252')\n",
    "    latlong_data = pd.read_csv(\"latlons.csv\", sep=\",\", encoding='cp1252')\n",
    "    addresses_data = pd.read_csv(\"addresses.csv\", sep=\",\", encoding='cp1252')\n",
    "    \n",
    "    # clean input data\n",
    "    def clean_data(df):\n",
    "\n",
    "        # set index\n",
    "        df = df.set_index('ticket_id')\n",
    "\n",
    "        # fix misformatted columns (floats to str)\n",
    "        df['violation_street_number'] = df['violation_street_number'].astype('str')\n",
    "        df['mailing_address_str_number'] = df['mailing_address_str_number'].astype('str')\n",
    "\n",
    "        # remove columns not available in the test data set\n",
    "        leaked_cols = df.columns.difference(test_data.columns)\n",
    "\n",
    "        if len(leaked_cols) > 0:\n",
    "            leaked_cols = leaked_cols.tolist()\n",
    "            leaked_cols.remove('compliance')\n",
    "            df = df.drop(leaked_cols, axis=1)\n",
    "\n",
    "        # remove 'not responsible' rows\n",
    "        if 'compliance' in df.columns:\n",
    "            df['compliance'] = df['compliance'].astype('str')\n",
    "            df = df[df[\"compliance\"] != \"nan\"]\n",
    "\n",
    "\n",
    "        # remove single-level variables (no information)\n",
    "        df = df.drop(['violation_zip_code', 'grafitti_status', 'clean_up_cost', 'state_fee', 'admin_fee',\n",
    "                      'non_us_str_code'], axis=1)\n",
    "\n",
    "        # clean & consolidate street address data\n",
    "\n",
    "        # convert address floats to strings\n",
    "        df['violation_street_number'] = df['violation_street_number'].astype('str')\n",
    "        df['mailing_address_str_number'] = df['mailing_address_str_number'].astype('str')\n",
    "\n",
    "        # create consolidated columns\n",
    "        df['mailing_address'] = df['mailing_address_str_number'] + ' ' + df['mailing_address_str_name']\n",
    "        df['mailing_address'] = df['mailing_address'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "        df['violation_address'] = df['violation_street_number'] + ' ' + df['violation_street_name']  \n",
    "\n",
    "        # remove old versions\n",
    "        df = df.drop(['mailing_address_str_number', 'mailing_address_str_name', 'violation_street_number', 'violation_street_name'], axis=1)\n",
    "\n",
    "        # add geolocation data\n",
    "        geo_address_data = pd.merge(addresses_data, latlong_data, how=\"left\", on=\"address\").set_index(\"ticket_id\")\n",
    "        df = pd.merge(df, geo_address_data, left_index=True, right_index=True)\n",
    "\n",
    "        # function to reassign dtypes\n",
    "        def set_dtypes(vars_list, df, dtype):\n",
    "            for var in vars_list:\n",
    "                df[var] = df[var].astype(dtype)\n",
    "            return(df)\n",
    "\n",
    "        # set datetime variables \n",
    "        date_vars = ['ticket_issued_date','hearing_date']\n",
    "        df = set_dtypes(date_vars, df, 'datetime64')\n",
    "\n",
    "        # get time interval between hearing and issued dates\n",
    "        df[\"days_until_hearing\"] = (df[\"hearing_date\"] - df[\"ticket_issued_date\"]).dt.days.astype('float64')\n",
    "\n",
    "        # change all strings to lowercase\n",
    "        for col in df: \n",
    "            if (df[col].dtype.name == 'object'):\n",
    "                df[col] = df[col].str.lower()\n",
    "                df[col] = df[col].str.replace(\"\\.0\",\"\")\n",
    "\n",
    "        # set category variables\n",
    "        cat_vars = ['agency_name', 'disposition']\n",
    "        df = set_dtypes(cat_vars, df, 'category')\n",
    "\n",
    "        # return cleaned data\n",
    "        return df\n",
    "\n",
    "    # transform data (fit_transform on training, transform on validation/test)\n",
    "    def transform_data(train_cleaned, test_cleaned): # add test df after\n",
    "\n",
    "        ### IMPUTE MISSING VALUES\n",
    "\n",
    "        # impute missing values\n",
    "        df_num = train_cleaned.select_dtypes(np.float64)\n",
    "        df_cat = train_cleaned.select_dtypes(['category', 'object', 'datetime64']).drop([\"compliance\"], axis=1)\n",
    "        #print(\"num_df\", df_num.shape)\n",
    "\n",
    "        #print(\"cat_df\", df_cat.shape)\n",
    "\n",
    "        # train imputer and save fit-transformed data \n",
    "        imp_cat = SimpleImputer(strategy=\"most_frequent\") \n",
    "        cat_imp = imp_cat.fit_transform(df_cat) # array\n",
    "        cat_imp_df = pd.DataFrame(cat_imp, columns=df_cat.columns, index=df_cat.index) # df form\n",
    "\n",
    "        # num (use median)\n",
    "        imp_num = SimpleImputer(strategy=\"median\") \n",
    "        num_imp = imp_num.fit_transform(df_num) # array\n",
    "        num_imp_df = pd.DataFrame(num_imp, columns=df_num.columns, index=df_num.index) # df form\n",
    "\n",
    "        ### ENCODE CATEGORICAL DATA - SET THOSE TO INCLUDE HERE ...\n",
    "\n",
    "        # choose features to include and set as category ...\n",
    "        category_features = [\"agency_name\", \"disposition\"]\n",
    "        for feature in category_features: \n",
    "            cat_imp_df[feature] = cat_imp_df[feature].astype(\"category\")  \n",
    "        cat_df = cat_imp_df[category_features] # vars that will be included in the analysis\n",
    "\n",
    "        #print(cat_df.shape)\n",
    "\n",
    "        # train encoder and save fit-transformed data\n",
    "        encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "        feature_arr = encoder.fit_transform(cat_df)\n",
    "        feature_names = encoder.get_feature_names(category_features)\n",
    "        df_cat_ohe =  pd.DataFrame(feature_arr.toarray(), columns=feature_names)\n",
    "\n",
    "        #print(df_cat_ohe.shape)\n",
    "\n",
    "        ### SCALE NUMERIC VALUES - USE MIN-MAX AS DEFAULT\n",
    "\n",
    "        # train scaler and save fit-transformed data\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_arr = scaler.fit_transform(num_imp_df)\n",
    "        num_scaled_df = pd.DataFrame(scaled_arr, columns=df_num.columns, index=df_num.index) # df form\n",
    "\n",
    "        #print(num_scaled_df.shape)\n",
    "        train_tr = pd.merge(num_scaled_df, df_cat_ohe, left_index=True, right_index=True) #.fillna(0)\n",
    "        # add compliance back in\n",
    "        train_tr = pd.merge(train_tr, train_cleaned[\"compliance\"], left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "        # After fitting all transformers: transform test data using trained imputers & transformers ...\n",
    "\n",
    "        # impute missing values\n",
    "        test_num = test_cleaned.select_dtypes(np.float64)\n",
    "        test_cat = test_cleaned.select_dtypes(['category', 'object', 'datetime64'])\n",
    "        #print(\"test_num\", test_num.shape)\n",
    "        #print(\"test_cat\", test_cat.shape)\n",
    "\n",
    "        # transform test data \n",
    "        cat_imp_test = imp_cat.transform(test_cat) # array\n",
    "        cat_imp_test = pd.DataFrame(cat_imp_test, columns=test_cat.columns, index=test_cat.index) # df form\n",
    "\n",
    "        # num (use median)\n",
    "        num_imp_test = imp_num.transform(test_num) # array\n",
    "        num_imp_test = pd.DataFrame(num_imp_test, columns=test_num.columns, index=test_num.index) # df form\n",
    "\n",
    "        ### ENCODE CATEGORICAL DATA - SET THOSE TO INCLUDE HERE ...\n",
    "\n",
    "        for feature in category_features: \n",
    "            cat_imp_test[feature] = cat_imp_test[feature].astype(\"category\")  \n",
    "        cat_test = cat_imp_test[category_features] # vars that will be included in the analysis\n",
    "\n",
    "        #print(cat_test.shape)\n",
    "\n",
    "        # transform test data\n",
    "        feature_arr = encoder.transform(cat_test)\n",
    "        feature_names = encoder.get_feature_names(category_features)\n",
    "        test_cat_ohe =  pd.DataFrame(feature_arr.toarray(), columns=feature_names, index=test_num.index)\n",
    "\n",
    "        #print(test_cat_ohe.shape)\n",
    "\n",
    "        ### SCALE NUMERIC VALUES - USE MIN-MAX AS DEFAULT\n",
    "\n",
    "        # transform test data\n",
    "        scaled_arr = scaler.transform(num_imp_test)\n",
    "        num_scaled_test = pd.DataFrame(scaled_arr, columns=test_num.columns, index=test_num.index) # df form\n",
    "\n",
    "        #print(num_scaled_test.shape)\n",
    "\n",
    "        test_tr = pd.merge(num_scaled_test, test_cat_ohe, left_index=True, right_index=True) #.fillna(0)\n",
    "\n",
    "        return train_tr, test_tr\n",
    "\n",
    "    def separate_X_from_y(train_tr, target_var):\n",
    "\n",
    "        y = train_tr[target_var]\n",
    "        X = train_tr.drop(target_var, axis=1)\n",
    "\n",
    "        return X,y\n",
    "    \n",
    "    # test with both train and test data ... \n",
    "    train_cleaned = clean_data(train_data)\n",
    "    test_cleaned = clean_data(test_data)\n",
    "    train_tr, test_tr = transform_data(train_cleaned, test_cleaned)\n",
    "    \n",
    "    # separate X, y from training data\n",
    "    X, y = separate_X_from_y(train_tr, \"compliance\")\n",
    "\n",
    "    # training_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    # isolate most important features\n",
    "    most_important_features = [\"lat\", \"lon\", \"days_until_hearing\", \"late_fee\", \"discount_amount\"]\n",
    "    X_train = X_train[most_important_features]\n",
    "    X_test = X_test[most_important_features]\n",
    "    \n",
    "    # run optimal model based on GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    best_model = RandomForestClassifier(max_features=3, min_samples_split=10).fit(X_train, y_train)\n",
    "    \n",
    "    # get the final set of predictions based on the test set\n",
    "    final_X = test_tr[most_important_features]\n",
    "    final_X[\"compliance\"] = [i[1] for i in best_model.predict_proba(final_X)]\n",
    "    answer = final_X[\"compliance\"].astype('float32')\n",
    "    \n",
    "    return answer # Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abd67a2-172e-4509-87fd-928e1728a03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id\n",
       "284932    0.026361\n",
       "285362    0.139672\n",
       "285361    0.012650\n",
       "285338    0.174310\n",
       "285346    0.053102\n",
       "            ...   \n",
       "376496    0.007123\n",
       "376497    0.007123\n",
       "376499    0.012500\n",
       "376500    0.012500\n",
       "369851    0.294283\n",
       "Name: compliance, Length: 61001, dtype: float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blight_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e59c3-d834-4b3b-84bc-c7c1def990df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
